{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Convert Bannerlord Lore Model to GGUF\n",
        "\n",
        "Run all cells in Google Colab (with GPU runtime) to convert your model to GGUF format for LM Studio/Ollama.\n",
        "\n",
        "1. Open this notebook in Google Colab\n",
        "2. Select Runtime -> Change runtime type -> T4 GPU\n",
        "3. Run all cells\n",
        "4. Download the generated GGUF file\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Install dependencies\n",
        "!pip install unsloth huggingface_hub -q\n",
        "print(\"Dependencies installed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Download LoRA v2 adapter from HuggingFace\n",
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "lora_id = \"TSEOsiris/bannerlord-lore-lora-v2\"\n",
        "lora_dir = \"./bannerlord-lore-lora\"\n",
        "\n",
        "print(f\"Downloading LoRA adapter: {lora_id}...\")\n",
        "snapshot_download(repo_id=lora_id, local_dir=lora_dir)\n",
        "print(\"LoRA adapter downloaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Load base model + LoRA adapter and convert to GGUF\n",
        "import os\n",
        "import gc\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "os.makedirs(\"bannerlord-lore-gguf\", exist_ok=True)\n",
        "\n",
        "# Load the same clean base model used for training\n",
        "base_model = \"unsloth/Qwen3-8B\"\n",
        "print(f\"Loading base model: {base_model}\")\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=base_model,\n",
        "    max_seq_length=2048,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "print(\"Base model loaded!\")\n",
        "\n",
        "# Load LoRA adapter\n",
        "from peft import PeftModel\n",
        "print(\"\\nApplying LoRA adapter...\")\n",
        "model = PeftModel.from_pretrained(model, lora_dir)\n",
        "print(\"LoRA adapter applied!\")\n",
        "\n",
        "# Free some memory\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Save as GGUF Q4_K_M\n",
        "print(\"\\nConverting to GGUF Q4_K_M format...\")\n",
        "print(\"This may take 5-10 minutes...\")\n",
        "\n",
        "model.save_pretrained_gguf(\n",
        "    \"bannerlord-lore-gguf\",\n",
        "    tokenizer,\n",
        "    quantization_method=\"q4_k_m\"\n",
        ")\n",
        "print(\"\\nGGUF conversion complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Download the GGUF file\n",
        "import os\n",
        "import glob\n",
        "from google.colab import files\n",
        "\n",
        "# List generated files\n",
        "print(\"Generated files:\")\n",
        "for f in os.listdir(\"bannerlord-lore-gguf\"):\n",
        "    size = os.path.getsize(f\"bannerlord-lore-gguf/{f}\") / (1024**3)\n",
        "    print(f\"  {f}: {size:.2f} GB\")\n",
        "\n",
        "# Download GGUF file\n",
        "gguf_files = glob.glob(\"bannerlord-lore-gguf/*.gguf\")\n",
        "if gguf_files:\n",
        "    print(f\"\\nDownloading {gguf_files[0]}...\")\n",
        "    files.download(gguf_files[0])\n",
        "else:\n",
        "    print(\"No GGUF file found - check previous cell for errors\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
