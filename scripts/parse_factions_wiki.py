#!/usr/bin/env python3
"""
–ü–∞—Ä—Å–∏–Ω–≥ wiki —Å—Ç—Ä–∞–Ω–∏—Ü —Ñ—Ä–∞–∫—Ü–∏–π –∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–æ–≤:
- Overview
- History
- Troops
- Tactics
- Economy

–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–µ JSON —Ñ–∞–π–ª—ã
"""

import json
import sys
import re
import time
from pathlib import Path
from typing import Dict, List, Optional, Any
from bs4 import BeautifulSoup, Tag, NavigableString

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∫–∏ –¥–ª—è Windows
if sys.platform == 'win32':
    import io
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

try:
    import requests
    from requests.adapters import HTTPAdapter
    from urllib3.util.retry import Retry
except ImportError:
    print("ERROR: requests not installed!")
    print("Please run: pip install requests")
    sys.exit(1)

try:
    from selenium import webdriver
    from selenium.webdriver.chrome.service import Service
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    USE_SELENIUM = True
    try:
        from webdriver_manager.chrome import ChromeDriverManager
        USE_WEBDRIVER_MANAGER = True
    except ImportError:
        USE_WEBDRIVER_MANAGER = False
except ImportError:
    USE_SELENIUM = False
    print("‚ö†Ô∏è  Selenium not available, will use requests only")


class FactionWikiParser:
    """–ü–∞—Ä—Å–µ—Ä wiki —Å—Ç—Ä–∞–Ω–∏—Ü —Ñ—Ä–∞–∫—Ü–∏–π"""
    
    def __init__(self, output_dir: Path, use_selenium=True):
        self.output_dir = output_dir
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.use_selenium = use_selenium and USE_SELENIUM
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ HTTP —Å–µ—Å—Å–∏–∏ —Å retry
        self.session = requests.Session()
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Selenium –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
        self.driver = None
        if self.use_selenium:
            try:
                chrome_options = Options()
                chrome_options.add_argument('--headless')
                chrome_options.add_argument('--no-sandbox')
                chrome_options.add_argument('--disable-dev-shm-usage')
                chrome_options.add_argument('--disable-blink-features=AutomationControlled')
                chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
                chrome_options.add_experimental_option('useAutomationExtension', False)
                chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
                
                if USE_WEBDRIVER_MANAGER:
                    service = Service(ChromeDriverManager().install())
                    self.driver = webdriver.Chrome(service=service, options=chrome_options)
                else:
                    self.driver = webdriver.Chrome(options=chrome_options)
                print("‚úÖ Selenium initialized")
            except Exception as e:
                print(f"‚ö†Ô∏è  Could not initialize Selenium: {e}")
                self.use_selenium = False
                self.driver = None
    
    def get_faction_wiki_url(self, faction_id: str, faction_name: str) -> Optional[str]:
        """–ü–æ–ª—É—á–∏—Ç—å wiki URL –¥–ª—è —Ñ—Ä–∞–∫—Ü–∏–∏"""
        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç URL –¥–ª—è Bannerlord wiki
        base_url = "https://bannerlord.fandom.com/wiki"
        
        # –ú–∞–ø–ø–∏–Ω–≥ ID —Ñ—Ä–∞–∫—Ü–∏–π –Ω–∞ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü
        faction_url_map = {
            'empire': 'Northern_Empire',
            'empire_w': 'Western_Empire',
            'empire_s': 'Southern_Empire',
            'khuzait': 'Khuzait_Khanate',
            'battania': 'Battania',
            'aserai': 'Aserai_Sultanate',
            'sturgia': 'Sturgia',
            'vlandia': 'Vlandia',
            'nord': 'Nord'
        }
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–∞–ø–ø–∏–Ω–≥ –µ—Å–ª–∏ –µ—Å—Ç—å
        if faction_id in faction_url_map:
            page_name = faction_url_map[faction_id]
        else:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º ID/–Ω–∞–∑–≤–∞–Ω–∏–µ –≤ —Ñ–æ—Ä–º–∞—Ç URL
            page_name = faction_name.replace(' ', '_')
            if not page_name:
                page_name = faction_id.replace('_', ' ').title().replace(' ', '_')
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
        possible_names = [
            page_name,
            faction_name.replace(' ', '_'),
            faction_id.replace('_', ' ').title().replace(' ', '_'),
            f"{page_name}_(Bannerlord)",
            f"{faction_name.replace(' ', '_')}_(Bannerlord)"
        ]
        
        for name in possible_names:
            url = f"{base_url}/{name}"
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ (–¥–µ–ª–∞–µ–º HEAD –∑–∞–ø—Ä–æ—Å)
            try:
                response = self.session.head(url, timeout=5, allow_redirects=True)
                if response.status_code == 200:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ –æ—à–∏–±–∫–∏
                    if 'not a valid community' not in response.text.lower():
                        return url
            except:
                continue
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ –≤–µ—Ä–æ—è—Ç–Ω—ã–π URL
        return f"{base_url}/{page_name}"
    
    def download_page(self, url: str) -> Optional[str]:
        """–°–∫–∞—á–∞—Ç—å HTML —Å—Ç—Ä–∞–Ω–∏—Ü—É"""
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º Selenium –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω (–¥–ª—è JavaScript)
        if self.use_selenium and self.driver:
            try:
                self.driver.get(url)
                # –ñ–¥–µ–º –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                WebDriverWait(self.driver, 10).until(
                    EC.presence_of_element_located((By.TAG_NAME, "body"))
                )
                time.sleep(2)  # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –≤—Ä–µ–º—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ JS
                return self.driver.page_source
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Error downloading with Selenium: {e}")
                # Fallback to requests
                pass
        
        # Fallback: –æ–±—ã—á–Ω—ã–π HTTP –∑–∞–ø—Ä–æ—Å
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            return response.text
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Error downloading {url}: {e}")
            return None
    
    def extract_section(self, soup: BeautifulSoup, section_title: str) -> Optional[str]:
        """–ò–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ —Ä–∞–∑–¥–µ–ª–∞ –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫—É"""
        # –ò—â–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ (h1, h2, h3, h4, span —Å –∫–ª–∞—Å—Å–æ–º mw-headline)
        headers = soup.find_all(['h1', 'h2', 'h3', 'h4'])
        
        # –¢–∞–∫–∂–µ –∏—â–µ–º span —Å –∫–ª–∞—Å—Å–æ–º mw-headline (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ Fandom)
        headlines = soup.find_all('span', class_='mw-headline')
        for headline in headlines:
            # –°–æ–∑–¥–∞–µ–º –ø—Å–µ–≤–¥–æ-–∑–∞–≥–æ–ª–æ–≤–æ–∫ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            parent = headline.parent
            if parent and parent.name in ['h1', 'h2', 'h3', 'h4']:
                if parent not in headers:
                    headers.append(parent)
        
        # –¢–∞–∫–∂–µ –∏—â–µ–º —á–µ—Ä–µ–∑ id (–∏–Ω–æ–≥–¥–∞ —Ä–∞–∑–¥–µ–ª—ã –∏–º–µ—é—Ç id="Overview" –∏ —Ç.–¥.)
        section_id = soup.find(id=section_title.lower())
        if section_id:
            parent_header = section_id.find_parent(['h1', 'h2', 'h3', 'h4'])
            if parent_header and parent_header not in headers:
                headers.append(parent_header)
        
        for header in headers:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ –Ω—É–∂–Ω—ã–π —Ç–µ–∫—Å—Ç
            header_text = header.get_text(strip=True).lower()
            headline_text = header.find('span', class_='mw-headline')
            if headline_text:
                header_text = headline_text.get_text(strip=True).lower()
            
            # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º id –∑–∞–≥–æ–ª–æ–≤–∫–∞
            header_id = header.get('id', '').lower()
            if section_title.lower() in header_text or section_title.lower() in header_id:
                # –°–æ–±–∏—Ä–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç –¥–æ —Å–ª–µ–¥—É—é—â–µ–≥–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Ç–æ–≥–æ –∂–µ –∏–ª–∏ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è
                section_content = []
                current = header.next_sibling
                
                header_level = int(header.name[1]) if header.name.startswith('h') else 3
                
                while current:
                    # –ï—Å–ª–∏ –≤—Å—Ç—Ä–µ—Ç–∏–ª–∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Ç–æ–≥–æ –∂–µ –∏–ª–∏ –±–æ–ª–µ–µ –≤—ã—Å–æ–∫–æ–≥–æ —É—Ä–æ–≤–Ω—è - —Å—Ç–æ–ø
                    if current.name and current.name.startswith('h'):
                        current_level = int(current.name[1])
                        if current_level <= header_level:
                            break
                    
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –Ω–∞–≤–∏–≥–∞—Ü–∏–æ–Ω–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –∏ —Ä–µ–∫–ª–∞–º—É
                    if isinstance(current, Tag):
                        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã –Ω–∞–≤–∏–≥–∞—Ü–∏–∏, –∏–Ω—Ñ–æ–±–æ–∫—Å—ã –∏ —Ç.–¥.
                        if current.get('class'):
                            classes = ' '.join(current.get('class', []))
                            if any(skip in classes.lower() for skip in ['navbox', 'infobox', 'mw-editsection', 'toc']):
                                current = current.next_sibling
                                continue
                        
                        text = current.get_text(separator=' ', strip=True)
                        # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ –∏–ª–∏ —Å–ª—É–∂–µ–±–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
                        if text and len(text) > 20 and 'please enable javascript' not in text.lower():
                            section_content.append(text)
                    elif isinstance(current, NavigableString):
                        text = str(current).strip()
                        if text and len(text) > 20:
                            section_content.append(text)
                    
                    current = current.next_sibling
                
                if section_content:
                    result = ' '.join(section_content).strip()
                    # –û—á–∏—â–∞–µ–º –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
                    result = re.sub(r'\s+', ' ', result)
                    return result
        
        return None
    
    def parse_faction_page(self, html_content: str) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É —Ñ—Ä–∞–∫—Ü–∏–∏ –∏ –∏–∑–≤–ª–µ—á—å –≤—Å–µ —Ä–∞–∑–¥–µ–ª—ã"""
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø–æ–ª—É—á–∏–ª–∏ –ª–∏ –º—ã —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å –æ—à–∏–±–∫–æ–π
        html_lower = html_content.lower()
        if ('not a valid community' in html_lower or 
            'page not found' in html_lower or 
            'this page does not exist' in html_lower or
            'does not have an article' in html_lower):
            print(f"   ‚ö†Ô∏è  Page not found or invalid")
            return {
                'overview': None,
                'history': None,
                'troops': None,
                'tactics': None,
                'economy': None
            }
        
        # –ù–∞–π—Ç–∏ –æ—Å–Ω–æ–≤–Ω—É—é –æ–±–ª–∞—Å—Ç—å –∫–æ–Ω—Ç–µ–Ω—Ç–∞ - –∏—â–µ–º –±–æ–ª–µ–µ —Ç–æ—á–Ω–æ
        content_area = None
        
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –ø–æ–∏—Å–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
        content_selectors = [
            ('div', {'class': 'mw-parser-output'}),
            ('div', {'id': 'content'}),
            ('main', {}),
            ('article', {}),
            ('div', {'class': 'page-content'}),
            ('div', {'class': 'WikiaPage'}),
        ]
        
        for tag, attrs in content_selectors:
            content_area = soup.find(tag, attrs)
            if content_area:
                break
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –∏—â–µ–º –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ - –∫–æ–Ω—Ç–µ–Ω—Ç –æ–±—ã—á–Ω–æ –º–µ–∂–¥—É header –∏ footer
        if not content_area:
            # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç —Å –∫–ª–∞—Å—Å–æ–º, —Å–æ–¥–µ—Ä–∂–∞—â–∏–º "content" –∏–ª–∏ "main"
            for div in soup.find_all('div'):
                classes = ' '.join(div.get('class', []))
                if 'content' in classes.lower() or 'main' in classes.lower():
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ —Ñ—É—Ç–µ—Ä –∏–ª–∏ –Ω–∞–≤–∏–≥–∞—Ü–∏—è
                    if 'footer' not in classes.lower() and 'nav' not in classes.lower():
                        content_area = div
                        break
        
        if not content_area:
            content_area = soup
        
        result = {
            'overview': None,
            'history': None,
            'troops': None,
            'tactics': None,
            'economy': None
        }
        
        # –û—Ç–ª–∞–¥–∫–∞: –≤—ã–≤–æ–¥–∏–º –≤—Å–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
        all_headers = content_area.find_all(['h1', 'h2', 'h3', 'h4'])
        header_texts = [h.get_text(strip=True) for h in all_headers[:15]]
        if header_texts:
            print(f"   üìã Found headers: {', '.join(header_texts[:8])}")
        
        # –¢–∞–∫–∂–µ –∏—â–µ–º span —Å –∫–ª–∞—Å—Å–æ–º mw-headline (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ Fandom)
        headlines = content_area.find_all('span', class_='mw-headline')
        if headlines:
            headline_texts = [h.get_text(strip=True) for h in headlines[:10]]
            print(f"   üìã Found headlines: {', '.join(headline_texts[:8])}")
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–∞–∂–¥—ã–π —Ä–∞–∑–¥–µ–ª
        result['overview'] = self.extract_section(content_area, 'Overview')
        result['history'] = self.extract_section(content_area, 'History')
        result['troops'] = self.extract_section(content_area, 'Troops')
        result['tactics'] = self.extract_section(content_area, 'Tactics')
        result['economy'] = self.extract_section(content_area, 'Economy')
        
        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —á–µ—Ä–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∏, –ø–æ–ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —á–µ—Ä–µ–∑ –∏–Ω—Ñ–æ–±–æ–∫—Å—ã –∏ —Ç–∞–±–ª–∏—Ü—ã
        if not result['overview']:
            # –ò—â–µ–º –ø–µ—Ä–≤—ã–π –ø–∞—Ä–∞–≥—Ä–∞—Ñ –ø–æ—Å–ª–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–Ω–æ –Ω–µ –≤ –∏–Ω—Ñ–æ–±–æ–∫—Å–µ)
            paragraphs = content_area.find_all('p')
            for p in paragraphs:
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã –≤ –∏–Ω—Ñ–æ–±–æ–∫—Å–∞—Ö, –Ω–∞–≤–∏–≥–∞—Ü–∏–∏ –∏ —Ñ—É—Ç–µ—Ä–∞—Ö
                parent = p.parent
                skip = False
                while parent:
                    if parent.get('class'):
                        classes = ' '.join(parent.get('class', []))
                        if any(skip_class in classes.lower() for skip_class in ['infobox', 'navbox', 'footer', 'navigation', 'sidebar']):
                            skip = True
                            break
                    if parent.name in ['footer', 'nav', 'header']:
                        skip = True
                        break
                    parent = parent.parent
                
                if skip:
                    continue
                
                text = p.get_text(separator=' ', strip=True)
                # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã, –Ω–æ –º–µ–Ω–µ–µ —Å—Ç—Ä–æ–≥–æ
                if (text and len(text) > 30 and 
                    'please enable javascript' not in text.lower() and
                    not text.lower().startswith('what is fandom') and
                    'terms of use' not in text.lower() and
                    'privacy policy' not in text.lower() and
                    'digital services act' not in text.lower()):
                    result['overview'] = text
                    break
        
        return result
    
    def parse_faction(self, faction_id: str, faction_name: str, wiki_url: Optional[str] = None) -> Dict[str, Any]:
        """–ü–∞—Ä—Å–∏—Ç—å —Ñ—Ä–∞–∫—Ü–∏—é"""
        print(f"\nüìñ Parsing faction: {faction_name} ({faction_id})")
        
        # –ü–æ–ª—É—á–∞–µ–º URL –µ—Å–ª–∏ –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω
        if not wiki_url:
            wiki_url = self.get_faction_wiki_url(faction_id, faction_name)
            print(f"   üîó Wiki URL: {wiki_url}")
        
        # –°–∫–∞—á–∏–≤–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
        html_content = self.download_page(wiki_url)
        if not html_content:
            print(f"   ‚ùå Failed to download page")
            return None
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –ø–æ–ª—É—á–∏–ª–∏ –ª–∏ –º—ã —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–µ–º JavaScript
        if 'please enable javascript' in html_content.lower():
            print(f"   ‚ö†Ô∏è  Page requires JavaScript, but Selenium not available or failed")
        
        # –ü–∞—Ä—Å–∏–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
        sections = self.parse_faction_page(html_content)
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = {
            'id': faction_id,
            'name': faction_name,
            'wiki_url': wiki_url,
            'sections': sections
        }
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã (–∏—Å–∫–ª—é—á–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è)
        found_sections = sum(1 for v in sections.values() 
                           if v and 'please enable javascript' not in v.lower() and len(v) > 20)
        print(f"   ‚úÖ Found {found_sections}/5 sections")
        
        return result
    
    def close(self):
        """–ó–∞–∫—Ä—ã—Ç—å –±—Ä–∞—É–∑–µ—Ä –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è Selenium"""
        if self.driver:
            self.driver.quit()
    
    def save_faction(self, faction_data: Dict[str, Any]):
        """–°–æ—Ö—Ä–∞–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ —Ñ—Ä–∞–∫—Ü–∏–∏ –≤ JSON"""
        if not faction_data:
            return
        
        filename = f"{faction_data['id']}.json"
        output_file = self.output_dir / filename
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(faction_data, f, ensure_ascii=False, indent=2)
        
        print(f"   üíæ Saved to {filename}")


def get_factions_from_db():
    """–ü–æ–ª—É—á–∏—Ç—å —Ñ—Ä–∞–∫—Ü–∏–∏ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —Å wiki_url"""
    import sqlite3
    project_root = Path(__file__).parent.parent
    db_path = project_root / 'Database' / 'bannerlord_lore.db'
    
    if not db_path.exists():
        return None
    
    conn = sqlite3.connect(str(db_path))
    conn.row_factory = sqlite3.Row
    cursor = conn.cursor()
    
    # –ü–æ–ª—É—á–∞–µ–º –∏–∑ kingdoms —Ç–∞–±–ª–∏—Ü—ã
    cursor.execute('''
        SELECT id, name, wiki_url
        FROM kingdoms
        WHERE wiki_url IS NOT NULL AND wiki_url != ''
    ''')
    
    factions = []
    for row in cursor.fetchall():
        factions.append({
            'id': row['id'],
            'name': row['name'],
            'wiki_url': row['wiki_url']
        })
    
    conn.close()
    return factions


def main():
    """Main entry point"""
    project_root = Path(__file__).parent.parent
    
    # –ü—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å —Ñ—Ä–∞–∫—Ü–∏–∏ –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
    factions = get_factions_from_db()
    
    # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å, –∑–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ JSON
    if not factions:
        factions_file = project_root / 'finetuning_data' / 'factions.json'
        if not factions_file.exists():
            print(f"‚ùå Factions file not found: {factions_file}")
            print("   Please run export_finetuning_data.py first")
            return
        
        with open(factions_file, 'r', encoding='utf-8') as f:
            factions = json.load(f)
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä—Å–µ—Ä
    output_dir = project_root / 'wiki_data' / 'factions'
    parser = FactionWikiParser(output_dir)
    
    print("=" * 60)
    print("PARSING FACTION WIKI PAGES")
    print("=" * 60)
    print(f"\nüìä Found {len(factions)} factions to parse")
    
    parsed_count = 0
    failed_count = 0
    
    try:
        for faction in factions:
            faction_id = faction.get('id', '')
            faction_name = faction.get('name_en', '') or faction.get('name', '')
            wiki_url = faction.get('wiki_url', '')
            
            if not faction_id or not faction_name:
                continue
            
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º wiki_url –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö, –µ—Å–ª–∏ –µ—Å—Ç—å
                result = parser.parse_faction(faction_id, faction_name, wiki_url if wiki_url else None)
                if result:
                    parser.save_faction(result)
                    parsed_count += 1
                else:
                    failed_count += 1
            except Exception as e:
                print(f"   ‚ùå Error parsing {faction_name}: {e}")
                failed_count += 1
            
            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            time.sleep(1)
    finally:
        # –ó–∞–∫—Ä—ã–≤–∞–µ–º –±—Ä–∞—É–∑–µ—Ä –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ Selenium
        parser.close()
    
    print("\n" + "=" * 60)
    print("‚úÖ PARSING COMPLETED!")
    print("=" * 60)
    print(f"\nüìä Statistics:")
    print(f"   Parsed: {parsed_count}")
    print(f"   Failed: {failed_count}")
    print(f"   Total: {len(factions)}")
    print(f"\nüìÅ Output directory: {output_dir}")


if __name__ == '__main__':
    main()

