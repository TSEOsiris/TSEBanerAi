# –ü–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ Fine-tuning LLM –¥–ª—è Bannerlord

## –≠–ø–æ–ø–µ—è –∑–∞ 3 –¥–Ω—è: –£—Ä–æ–∫–∏ –∏ —Ä–µ—à–µ–Ω–∏—è

---

## üìã –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

1. [–û–±–∑–æ—Ä –ø—Ä–æ–µ–∫—Ç–∞](#–æ–±–∑–æ—Ä-–ø—Ä–æ–µ–∫—Ç–∞)
2. [–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞](#–ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞-–¥–∞—Ç–∞—Å–µ—Ç–∞)
3. [–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏](#–æ–±—É—á–µ–Ω–∏–µ-–º–æ–¥–µ–ª–∏)
4. [–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ GGUF](#–∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è-–≤-gguf)
5. [–ü—Ä–æ–±–ª–µ–º—ã –∏ —Ä–µ—à–µ–Ω–∏—è](#–ø—Ä–æ–±–ª–µ–º—ã-–∏-—Ä–µ—à–µ–Ω–∏—è)
6. [–û–±—É—á–µ–Ω–∏–µ –≤ Colab](#–æ–±—É—á–µ–Ω–∏–µ-–≤-colab)

---

## üìä –û–±–∑–æ—Ä –ø—Ä–æ–µ–∫—Ç–∞

### –ß—Ç–æ –º—ã —Å–¥–µ–ª–∞–ª–∏:
- Fine-tuned **Qwen3-8B** –Ω–∞ –ª–æ—Ä–µ Mount & Blade II: Bannerlord
- –°–æ–∑–¥–∞–ª–∏ –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ 7,209 –∑–∞–ø–∏—Å–µ–π –Ω–∞ 3 —è–∑—ã–∫–∞—Ö (EN, RU, TR)
- –û–±—É—á–∏–ª–∏ –º–æ–¥–µ–ª—å –Ω–∞ 2000 —à–∞–≥–æ–≤
- –°–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–ª–∏ –≤ GGUF –¥–ª—è LM Studio/Ollama

### –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–∞ HuggingFace:
- **–î–∞—Ç–∞—Å–µ—Ç:** https://huggingface.co/datasets/TSEOsiris/bannerlord-lore-dataset
- **LoRA v1 (4-bit base):** https://huggingface.co/TSEOsiris/bannerlord-lore-lora
- **LoRA v2 (clean base):** https://huggingface.co/TSEOsiris/bannerlord-lore-lora-v2
- **FP16 –º–æ–¥–µ–ª—å:** https://huggingface.co/TSEOsiris/bannerlord-lore-fp16
- **GGUF:** https://huggingface.co/TSEOsiris/bannerlord-lore-fp16-Q4_K_M-GGUF

---

## üìö –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞

### –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö:
1. In-game Encyclopedia (–≥–µ—Ä–æ–∏, –∫–æ—Ä–æ–ª–µ–≤—Å—Ç–≤–∞, –ø–æ—Å–µ–ª–µ–Ω–∏—è)
2. "Travels in Calradia" –Ω–æ–≤–µ–ª–ª—ã
3. –û–ø–∏—Å–∞–Ω–∏—è —Ñ—Ä–∞–∫—Ü–∏–π –∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–π
4. –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Ñ–∏–≥—É—Ä—ã (Emperor Neretzes)

### –§–æ—Ä–º–∞—Ç –¥–∞—Ç–∞—Å–µ—Ç–∞ (Alpaca):
```json
{
  "instruction": "Tell me about the Battanian faction",
  "input": "",
  "output": "The Battanians are the forest people..."
}
```

### –°–∫—Ä–∏–ø—Ç—ã –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏:
- `scripts/parse_ingame_encyclopedia.py` ‚Äî –ø–∞—Ä—Å–∏–Ω–≥ —ç–Ω—Ü–∏–∫–ª–æ–ø–µ–¥–∏–∏
- `scripts/prepare_unsloth_dataset.py` ‚Äî –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç –æ–±—É—á–µ–Ω–∏—è
- `scripts/publish_dataset.py` ‚Äî –ø—É–±–ª–∏–∫–∞—Ü–∏—è –Ω–∞ HuggingFace

---

## üéØ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏

### –ñ–µ–ª–µ–∑–æ:
- **–õ–æ–∫–∞–ª—å–Ω–æ:** RTX 4070 Super (12GB) ‚Äî –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
- **Colab:** A100 (40GB) ‚Äî –Ω—É–∂–µ–Ω –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ GGUF

### –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è:
```python
model_name = "unsloth/Qwen3-8B"  # –í–ê–ñ–ù–û: —á–∏—Å—Ç–∞—è –º–æ–¥–µ–ª—å!
max_steps = 2000
batch_size = 1  # –¥–ª—è 12GB GPU
gradient_accumulation_steps = 8
learning_rate = 2e-4
lora_r = 16
lora_alpha = 16
```

### –ö–æ–º–∞–Ω–¥–∞ –∑–∞–ø—É—Å–∫–∞:
```powershell
cd C:\TSEBanerAi\TSEBanerAi
.\venv_py312\Scripts\Activate.ps1
python scripts/train_unsloth_v2.py --max_steps 2000 --batch_size 1
```

### –í—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è:
- 2000 —à–∞–≥–æ–≤ √ó ~5 —Å–µ–∫ = **~3 —á–∞—Å–∞** –Ω–∞ RTX 4070 Super

---

## üîÑ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ GGUF

### ‚ö†Ô∏è –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û:

**–ù–µ–ª—å–∑—è –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –≤ GGUF –º–æ–¥–µ–ª—å, –æ–±—É—á–µ–Ω–Ω—É—é –Ω–∞ `unsloth/Qwen3-8B-unsloth-bnb-4bit`!**

–ü—Ä–∏—á–∏–Ω–∞: bitsandbytes –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è —Å–æ–∑–¥–∞—ë—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–µ–Ω–∑–æ—Ä—ã (`.absmax`, `.quant_state`), –∫–æ—Ç–æ—Ä—ã–µ llama.cpp –Ω–µ –ø–æ–Ω–∏–º–∞–µ—Ç.

### ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—É—Ç—å:

1. **–û–±—É—á–µ–Ω–∏–µ:** –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `unsloth/Qwen3-8B` (–±–µ–∑ `-bnb-4bit`)
2. **–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ Colab —Å A100:**

```python
# –ó–∞–≥—Ä—É–∑–∏—Ç—å –ü–û–õ–ù–£–Æ FP16 –º–æ–¥–µ–ª—å (–Ω–µ 4-bit!)
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen3-8B",
    torch_dtype=torch.bfloat16,
    device_map="auto",
)
tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-8B")

# –ü—Ä–∏–º–µ–Ω–∏—Ç—å LoRA
from peft import PeftModel
model = PeftModel.from_pretrained(model, "–ø—É—Ç—å/–∫/lora")
model = model.merge_and_unload()

# –°–æ—Ö—Ä–∞–Ω–∏—Ç—å —á–∏—Å—Ç—É—é FP16
model.save_pretrained("output")
tokenizer.save_pretrained("output")

# –ó–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∞ HuggingFace
# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å https://huggingface.co/spaces/ggml-org/gguf-my-repo
```

---

## üîß –ü—Ä–æ–±–ª–µ–º—ã –∏ —Ä–µ—à–µ–Ω–∏—è

### –ü—Ä–æ–±–ª–µ–º–∞ 1: Python 3.14 –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç CUDA
**–†–µ—à–µ–Ω–∏–µ:** –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Python 3.12

### –ü—Ä–æ–±–ª–µ–º–∞ 2: –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∞–µ—Ç
**–†–µ—à–µ–Ω–∏–µ:** –£–º–µ–Ω—å—à–∏—Ç—å batch_size –¥–æ 1, —É–≤–µ–ª–∏—á–∏—Ç—å gradient_accumulation

### –ü—Ä–æ–±–ª–µ–º–∞ 3: "bitsandbytes not supported" –ø—Ä–∏ GGUF –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
**–†–µ—à–µ–Ω–∏–µ:** 
1. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —á–∏—Å—Ç—É—é –±–∞–∑–æ–≤—É—é –º–æ–¥–µ–ª—å (–±–µ–∑ `-bnb-4bit`)
2. –ó–∞–≥—Ä—É–∂–∞—Ç—å FP16 –º–æ–¥–µ–ª—å –¥–ª—è merge
3. –£–¥–∞–ª—è—Ç—å `quantization_config` –∏–∑ config.json

### –ü—Ä–æ–±–ª–µ–º–∞ 4: T4/RTX 4070 –Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç –ø–∞–º—è—Ç–∏ –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏
**–†–µ—à–µ–Ω–∏–µ:** –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å A100 –≤ Colab Pro ($10/–º–µ—Å—è—Ü)

### –ü—Ä–æ–±–ª–µ–º–∞ 5: "config.json not found" –ø—Ä–∏ GGUF
**–†–µ—à–µ–Ω–∏–µ:** –°–Ω–∞—á–∞–ª–∞ `model.save_pretrained()`, –ø–æ—Ç–æ–º –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è

### –ü—Ä–æ–±–ª–µ–º–∞ 6: Unsloth –ø—ã—Ç–∞–µ—Ç—Å—è –∑–∞–ø—É—Å—Ç–∏—Ç—å apt-get –Ω–∞ Windows
**–†–µ—à–µ–Ω–∏–µ:** GGUF –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ –≤ Linux (Colab/Kaggle)

---

## ‚òÅÔ∏è –û–±—É—á–µ–Ω–∏–µ –≤ Colab

### –ú–æ–∂–Ω–æ –ª–∏ –æ–±—É—á–∞—Ç—å –≤ Colab?

**–î–ê!** Colab –æ—Ç–ª–∏—á–Ω–æ –ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.

### –ü–ª—é—Å—ã:
- –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π T4 (15GB) ‚Äî –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è 8B –º–æ–¥–µ–ª–∏
- Colab Pro –¥–∞—ë—Ç A100 (40GB) ‚Äî –∫–æ–º—Ñ–æ—Ä—Ç–Ω–æ –¥–ª—è –≤—Å–µ–≥–æ
- –ù–µ –Ω–∞–≥—Ä—É–∂–∞–µ—Ç –ª–æ–∫–∞–ª—å–Ω—ã–π –ü–ö
- –ú–æ–∂–Ω–æ –æ—Å—Ç–∞–≤–∏—Ç—å –Ω–∞ –Ω–æ—á—å

### –ú–∏–Ω—É—Å—ã:
- –°–µ—Å—Å–∏—è –æ—Ç–∫–ª—é—á–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ ~12 —á–∞—Å–æ–≤
- –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π Colab –º–æ–∂–µ—Ç –æ—Ç–∫–ª—é—á–∏—Ç—å –ø—Ä–∏ –≤—ã—Å–æ–∫–æ–π –Ω–∞–≥—Ä—É–∑–∫–µ
- –ù—É–∂–µ–Ω –∏–Ω—Ç–µ—Ä–Ω–µ—Ç

### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:
1. **–û–±—É—á–µ–Ω–∏–µ:** –ú–æ–∂–Ω–æ –≤ Colab (T4 –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ) –∏–ª–∏ –ª–æ–∫–∞–ª—å–Ω–æ
2. **–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ GGUF:** –¢–æ–ª—å–∫–æ Colab —Å A100 (–¥–ª—è 8B –º–æ–¥–µ–ª–∏)

### –ö–æ–¥ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –≤ Colab:

```python
!pip install unsloth -q

from unsloth import FastLanguageModel

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="unsloth/Qwen3-8B",
    max_seq_length=2048,
    load_in_4bit=True,
)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ LoRA
model = FastLanguageModel.get_peft_model(
    model,
    r=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",
                    "gate_proj", "up_proj", "down_proj"],
    lora_alpha=16,
)

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
from datasets import load_dataset
dataset = load_dataset("TSEOsiris/bannerlord-lore-dataset")

# –û–±—É—á–µ–Ω–∏–µ...
```

---

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

```
TSEBanerAi/
‚îú‚îÄ‚îÄ finetuning_data/           # –î–∞—Ç–∞—Å–µ—Ç—ã
‚îÇ   ‚îú‚îÄ‚îÄ unsloth_training_dataset.json
‚îÇ   ‚îú‚îÄ‚îÄ encyclopedia_*.json
‚îÇ   ‚îî‚îÄ‚îÄ travels_calradia/
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ train_unsloth_v2.py    # –û—Å–Ω–æ–≤–Ω–æ–π —Å–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è
‚îÇ   ‚îú‚îÄ‚îÄ test_finetuned_model.py
‚îÇ   ‚îú‚îÄ‚îÄ publish_dataset.py
‚îÇ   ‚îî‚îÄ‚îÄ Convert_to_GGUF_Colab.ipynb
‚îú‚îÄ‚îÄ outputs_v2/                # LoRA –∞–¥–∞–ø—Ç–µ—Ä
‚îú‚îÄ‚îÄ models/                    # –õ–æ–∫–∞–ª—å–Ω—ã–µ –º–æ–¥–µ–ª–∏
‚îî‚îÄ‚îÄ docs/
    ‚îî‚îÄ‚îÄ FULL_FINETUNING_GUIDE.md
```

---

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç (–¥–ª—è –±—É–¥—É—â–µ–≥–æ)

### –ï—Å–ª–∏ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–æ–±—É—á–∏—Ç—å:

1. –ü–æ–¥–≥–æ—Ç–æ–≤—å –¥–∞–Ω–Ω—ã–µ –≤ `finetuning_data/`
2. –ó–∞–ø—É—Å—Ç–∏ –ª–æ–∫–∞–ª—å–Ω–æ:
   ```powershell
   python scripts/train_unsloth_v2.py --max_steps 2000
   ```
3. –ó–∞–≥—Ä—É–∑–∏ LoRA –Ω–∞ HuggingFace
4. –í Colab (A100) –ø—Ä–∏–º–µ–Ω–∏ LoRA –∫ FP16 –º–æ–¥–µ–ª–∏ –∏ —Å–æ—Ö—Ä–∞–Ω–∏
5. –ò—Å–ø–æ–ª—å–∑—É–π Space –¥–ª—è GGUF –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏

### –ï—Å–ª–∏ –Ω—É–∂–Ω–∞ —Ç–æ–ª—å–∫–æ GGUF –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è:

–ò—Å–ø–æ–ª—å–∑—É–π –≥–æ—Ç–æ–≤—ã–π –∫–æ–¥ –∏–∑ —Ä–∞–∑–¥–µ–ª–∞ "–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ GGUF".

---

## üìù –ß–µ–∫–ª–∏—Å—Ç

- [ ] Python 3.12 (–Ω–µ 3.14!)
- [ ] CUDA 12.x
- [ ] –ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å: `unsloth/Qwen3-8B` (–Ω–µ `-bnb-4bit`)
- [ ] batch_size=1 –¥–ª—è 12GB GPU
- [ ] –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ GGUF —Ç–æ–ª—å–∫–æ –Ω–∞ A100 –≤ Colab
- [ ] FP16 –º–æ–¥–µ–ª—å –¥–ª—è merge (–Ω–µ 4-bit!)

---

## üéÆ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ GGUF

### LM Studio:
1. –°–∫–∞—á–∞–π GGUF —Å HuggingFace
2. –ü–æ–ª–æ–∂–∏ –≤ `~/.cache/lm-studio/models/`
3. –í—ã–±–µ—Ä–∏ –º–æ–¥–µ–ª—å –≤ LM Studio

### Ollama:
```bash
# –°–æ–∑–¥–∞–π Modelfile
FROM ./bannerlord-lore-fp16-Q4_K_M.gguf

# –°–æ–∑–¥–∞–π –º–æ–¥–µ–ª—å
ollama create bannerlord -f Modelfile

# –ò—Å–ø–æ–ª—å–∑—É–π
ollama run bannerlord "Tell me about Battania"
```

---

---

## üîÆ –ü–ª–∞–Ω—ã –Ω–∞ –±—É–¥—É—â–µ–µ

### –ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–µ –º–æ–¥–µ–ª–∏:

| –Ø–∑—ã–∫ | –ë–∞–∑–∞ –¥–ª—è fine-tuning | –°—Ç–∞—Ç—É—Å |
|------|---------------------|--------|
| EN | Qwen3-8B | ‚úÖ –ì–æ—Ç–æ–≤–æ |
| RU | Saiga-Llama3-8B –∏–ª–∏ Vikhr-Nemo-12B | üìã –ü–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è |
| TR | –¢—É—Ä–µ—Ü–∫–∞—è –º–æ–¥–µ–ª—å (TBD) | üìã –ü–ª–∞–Ω–∏—Ä—É–µ—Ç—Å—è |

### –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∏–∑ –ø—Ä–æ—Ç–æ—Ç–∏–ø–∞:

1. –õ–æ–≥–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ –¥–∏–∞–ª–æ–≥–∏ Game Director
2. –û—Ç–±–∏—Ä–∞—Ç—å –ª—É—á—à–∏–µ –ø—Ä–∏–º–µ—Ä—ã (–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–π RP)
3. –î–æ–±–∞–≤–ª—è—Ç—å –≤ –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ fine-tuning
4. –ò—Ç–µ—Ä–∞—Ç–∏–≤–Ω–æ —É–ª—É—á—à–∞—Ç—å –º–æ–¥–µ–ª—å

### –§–æ—Ä–º–∞—Ç –ª–æ–≥–æ–≤ –¥–ª—è —Å–±–æ—Ä–∞:
```json
{
  "context": "Location: Marunath...",
  "player_input": "I seek employment",
  "model_response": "Caladog speaks...",
  "quality_rating": 5,
  "commands_generated": ["offer_contract"],
  "language": "en"
}
```

---

*–î–æ–∫—É–º–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω: 3 —è–Ω–≤–∞—Ä—è 2026*
*–ü–æ—Å–ª–µ 3 –¥–Ω–µ–π —ç–ø–∏—á–Ω–æ–π –±–æ—Ä—å–±—ã —Å fine-tuning –∏ GGUF –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–µ–π* üòÖ

